{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eigen_vec(xtrain, iteration, threshold):\n",
    "    x_square = np.dot(xtrain.T, xtrain)\n",
    "    vec = np.random.rand(x_square.shape[1])\n",
    "    vec_norm = vec/np.linalg.norm(vec)\n",
    "    #print(vec_norm)\n",
    "    count = 0\n",
    "    for i in range(iteration):\n",
    "        count += 1\n",
    "        u_vec = np.dot(x_square, vec_norm)\n",
    "        print(u_vec)\n",
    "        u_vec_norm = u_vec/np.linalg.norm(u_vec)\n",
    "        error = np.dot(u_vec_norm, vec_norm)\n",
    "        \n",
    "        vec_norm = u_vec_norm\n",
    "        \n",
    "        if (abs(error) > 1 - threshold):\n",
    "            #print(count)\n",
    "            #print(error)\n",
    "            #print(vec_norm)\n",
    "            return vec_norm    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svd(xtrain, n_component, threshold):\n",
    "    x = xtrain.copy()\n",
    "    svd_list=[]\n",
    "    for i in range(n_component):\n",
    "        #print(x.head())\n",
    "        eig_vec = eigen_vec(x, 2000, threshold)\n",
    "        eig_val = np.linalg.norm(x.dot(eig_vec))\n",
    "        print(eig_val, eig_vec)\n",
    "        svd_list.append((eig_val, eig_vec))\n",
    "        #subtracting Rank1 matrix from original matrix to get the second orthogonal vector \n",
    "        x -= (x.dot(eig_vec)[:,None])*eig_vec\n",
    "    #a, b = [np.array(x) for x in list(zip*(svd_list))]\n",
    "    return np.array(list(zip(*svd_list))[0]), np.array(list(zip(*svd_list))[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "#from data science from scratch book \n",
    "def sgd_eigen_vec(X, l_rate = 0.01):\n",
    "    X_demean = X.values -np.mean(X).values\n",
    "    ran_vec = np.random.rand(X.shape[1])\n",
    "    vec_norm = ran_vec/np.linalg.norm(ran_vec)\n",
    "    print(vec_norm)\n",
    "    variance = (X_demean.dot(vec_norm)).T.dot(X_demean.dot(vec_norm))/X_demean.shape[0]\n",
    "    update_epoch = [1]\n",
    "    i = 0\n",
    "    for epoch in range(50):\n",
    "        #x = random.sample(X_demean, 10)\n",
    "        w = vec_norm\n",
    "        old_variance = variance\n",
    "        variance_epoch = (X_demean.dot(w)).T.dot(X_demean.dot(w))/X_demean.shape[0]\n",
    "        variance += variance_epoch\n",
    "        #w = w +l_rate*(row.dot(w))*row\n",
    "        update = (l_rate*(X_demean.T.dot(X_demean.dot(w))*2 - 2*w))\n",
    "        update_epoch.append(np.linalg.norm(update))\n",
    "        vec_norm = w + update\n",
    "        vec_norm = vec_norm/np.linalg.norm(vec_norm)\n",
    "            \n",
    "            #print((update_row - update[-2]))\n",
    "        if (abs((update_epoch[-1]) - update_epoch[-2])) < 1e-3:\n",
    "            return epoch, vec_norm\n",
    "            \n",
    "        #print(vec_norm)\n",
    "#         print(update_epoch)\n",
    "#         print(update_epoch[-1], update_epoch[-2])\n",
    "#         print(((update_epoch[-1]) - update_epoch[-2]))\n",
    "    return (w, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/peisuke/vr_pca/blob/master/vr_pca.py\n",
    "# this is Oja implementation of power method \n",
    "def vr_pca(X, m, eta, rate=1e-5):\n",
    "    n, d = X.shape\n",
    "    w_t = np.random.rand(d) - 0.5\n",
    "    w_t = w_t / np.linalg.norm(w_t)\n",
    "    print(w_t)\n",
    "    for s in range(10):\n",
    "        u_t = X.T.dot(X.dot(w_t)) / n\n",
    "        print(u_t)\n",
    "        w = w_t\n",
    "        \n",
    "\n",
    "        for t in range(m):\n",
    "            print(w_t)\n",
    "            print(w)\n",
    "            i = np.random.randint(n)\n",
    "            _w = w + eta * (X[i] * (X[i].T.dot(w) - X[i].T.dot(w_t)) + u_t)\n",
    "            _w = _w / np.linalg.norm(_w)\n",
    "            w = _w\n",
    "\n",
    "        d = np.linalg.norm(w_t - w)\n",
    "        w_t = w\n",
    "\n",
    "        if d < rate:\n",
    "            return w_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(x,y):\n",
    "    pass\n",
    "\n",
    "\n",
    "def predict1(x_train, y_train, x_test, neighbors):\n",
    "    ##have to include for equal number of both classes for certain k\n",
    "    # how to make it for multiclass\n",
    "    # how to incorporate threshold in this\n",
    "    \n",
    "    sorted_distance = []\n",
    "    for i in x_test.values:\n",
    "        i = i.reshape(-1, i.shape[0])\n",
    "#         distance =[]\n",
    "#         for index, j in enumerate(x_train.values):\n",
    "        euclidean_dist = cdist(x_train.values, i)\n",
    "        #euclidean_dist= euclidean_dist.reshape(euclidean_dist.shape[1], -1)\n",
    "        euclidean_index = [(i,j) for i, j in list(zip((euclidean_dist), x_train.index))]\n",
    "        #print(euclidean_dist)\n",
    "#         print(list(zip((euclidean_dist), x_train.index)))\n",
    "        #print(x_train.index)\n",
    "        sorted_distance.append(sorted(euclidean_index))\n",
    "            #distance.append([euclidean_dist, index])\n",
    "        #sorted_distance.append(sorted(distance))\n",
    "   # print(sorted_distance)\n",
    "    \n",
    "    prediction =[]\n",
    "   \n",
    "    for i in sorted_distance:\n",
    "        target = []\n",
    "        for j in range(neighbors):\n",
    "            index = i[j][1]\n",
    "            target.append(y_train[index])\n",
    "        print(target)\n",
    "            \n",
    "        a = (Counter(target).most_common(1)) #replace with np.mean for knregressor\n",
    "#         print(target)\n",
    "        #print(a)\n",
    "        prediction.append(a)\n",
    "    #return (sorted_distance, target, prediction)\n",
    "    return prediction\n",
    "\n",
    "#def accuracy_score():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = []\n",
    "def score(x_train, y_train, x_test, y_test):\n",
    "    predicted = []\n",
    "    predicted = predict1(x_train, y_train, x_test, 13)\n",
    "    print(len(y_test), len(predicted))\n",
    "    TP = np.sum(np.logical_and(np.array(y_test) == 1, np.array(predicted) == 1))\n",
    "    TN = np.sum(np.logical_and(np.array(y_test) == 0, np.array(predicted) == 0))\n",
    "    FP = np.sum(np.logical_and(np.array(y_test) == 0, np.array(predicted) == 1))\n",
    "    FN = np.sum(np.logical_and(np.array(y_test) == 1, np.array(predicted) == 0))\n",
    "    return float(TP + TN)/float(TP + TN + FP + FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NotFittedError(Exception):\n",
    "    pass\n",
    "\n",
    "class LogisticClassifier():\n",
    "    def __init__(self, l_rate = 0.3, threshold = 0.5, fit_called=False):\n",
    "        self.fit_called = fit_called\n",
    "        self.coef = 0\n",
    "        self.bias = 0\n",
    "        self.threshold = threshold\n",
    "        self.l_rate = l_rate\n",
    "    \n",
    "    def fit(self, xtrain, ytrain, n_epoch):\n",
    "        self.fit_called = True\n",
    "        self.coef = np.array([0.0 for i in range(xtrain.shape[1])])\n",
    "        for epoch in range(n_epoch):\n",
    "                #sum_error = 0\n",
    "            xtrain = xtrain.sample(frac=1).reset_index(drop=True)\n",
    "            for index, row in enumerate(xtrain.values):\n",
    "                    #print(self.bias)\n",
    "                yhat = self.predict_one(row)\n",
    "                delta = (yhat - ytrain.iloc[index])\n",
    "                self.bias -= l_rate*delta\n",
    "                self.coef -= l_rate*delta*row\n",
    "                    #print(self.bias, self.coef)\n",
    "            #print('>epoch=%d, lrate=%.3f' % (epoch, l_rate))\n",
    "        return self.coef, self.bias\n",
    "    \n",
    "    def predict_one(self, row):\n",
    "        if not self.fit_called:\n",
    "            raise NotFittedError()\n",
    "        else:\n",
    "            return float(1.0 / (1.0 + np.exp(-self.coef.dot(row) - self.bias)))\n",
    "    \n",
    "#     def predict(self, xtest_array):\n",
    "#         if not self.fit_called:\n",
    "#             raise NotFittedError()\n",
    "#         else:\n",
    "#             predictions = []\n",
    "#             for i in range(len(xtest_array)):\n",
    "#                 xtest = xtest_array.iloc[i]\n",
    "#                 prediction = int(round(self.predict_one(xtest)))\n",
    "#                 predictions.append(prediction)\n",
    "#             return np.array(predictions)\n",
    "        \n",
    "    def predict(self, xtest_array):\n",
    "        if not self.fit_called:\n",
    "            raise NotFittedError()\n",
    "        else:\n",
    "            #predictions = []\n",
    "            predictions = np.array([])\n",
    "            for i in range(len(xtest_array)):\n",
    "                xtest = xtest_array.iloc[i]\n",
    "                prediction = self.predict_one(xtest)\n",
    "                #predictions.append(prediction)\n",
    "                predictions = np.appends(predictions, prediction)\n",
    "            #predictions = np.array(predictions)\n",
    "            #predict_mask = [0 if i<self.threshold else 1 for i in predictions]\n",
    "            predict_mask = predictions<self.threshold\n",
    "            return predict_mask*1\n",
    "            #return predictions\n",
    "            \n",
    "        \n",
    "            \n",
    "    def coefficients_gd(self, xtrain, ytrain, l_rate, n_epoch):\n",
    "        if not self.fit_called:\n",
    "            raise NotFittedError()\n",
    "        else:\n",
    "            self.coef = np.array([0.0 for i in range(xtrain.shape[1])])\n",
    "            for epoch in range(n_epoch):\n",
    "                #sum_error = 0\n",
    "                for index, row in enumerate(xtrain.values):\n",
    "                    #print(self.bias)\n",
    "                    yhat = self.predict_one(row)\n",
    "                    delta = (yhat - ytrain.iloc[index])\n",
    "                    self.bias -= l_rate*delta\n",
    "                    self.coef -= l_rate*delta*row\n",
    "                    #print(self.bias, self.coef)\n",
    "            #print('>epoch=%d, lrate=%.3f' % (epoch, l_rate))\n",
    "        return self.coef, self.bias\n",
    "\n",
    "    def acc_score(self, x_test, y_test):\n",
    "        yact_ypred = list(zip(y_test, self.predict(x_test)))\n",
    "        TP = yact_ypred.count((1,1))/len(x_test)\n",
    "        TN = yact_ypred.count((0,0))/len(x_test)\n",
    "        FP = yact_ypred.count((0,1))/len(x_test)\n",
    "        FN = yact_ypred.count((1,0))/len(x_test)\n",
    "        return float(TP + TN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gus function for gradient descent\n",
    "def _get_grad(X, y):\n",
    "        '''\n",
    "        Computes gradient of X_entropy loss function wrt B parameters for binary logistic regression (without bias). \n",
    "        '''\n",
    "        #forward pass---------------\n",
    "        p = logistic_func(beta, X)\n",
    "        #backward pass -------------\n",
    "        dloss = 1\n",
    "        p_sub = np.where(y, p, 1-p) #take complement where class is 0\n",
    "        print(p_sub)\n",
    "        dp = np.where(y, -1/p_sub, 1/p_sub) * dloss #backprop through Xentropy\n",
    "        dlogit = p * (1 - p) * dp #through sigmoid\n",
    "        dB = np.dot(X.T, dlogit) #through dot product\n",
    "        \n",
    "        return dB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLogisticRegression:\n",
    "\n",
    "    def __init__(self, x, y, tolerence = 0.00001):\n",
    "        self.tolerence = tolerence\n",
    "        self.cost = []\n",
    "        self.alpha = 0.1\n",
    "        self.lambd = 0.25\n",
    "        self.iter = 2500\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "        #initialie theta\n",
    "        self.theta = np.random.rand(x.shape[1],1)\n",
    "        \n",
    "\t#the cost function\n",
    "    def cost_fn(self, m):\n",
    "        h = self.sigmoid_function(np.dot(self.x, self.theta)) #get the hypothesis\n",
    "        J = ( 1.0/m ) * np.sum( -self.y * (np.log (h)) - ( 1.0 -self.y ) * (np.log(1.0 -h)) ) #This is the cost that needs to be paid by the learning algorithm if the outcome is hÎ¸(x) and the actual outcome is y\n",
    "        return J\n",
    "\n",
    "\t#The sigmoid function\n",
    "    def sigmoid_function(z):\n",
    "        return 1.0 / ( 1.0 + math.e**(-1*z) ) #Using 1.0 to make it a floating point type\n",
    "\n",
    "\t#Gradient function\n",
    "    def gradients(self, m):\n",
    "        zrd = self.theta\n",
    "        zrd[0, :] = 0\n",
    "        h = self.sigmoid_function(np.dot(self.x, self.theta)) #get the hypothesis\n",
    "        return ( 1.0/m ) * np.dot(self.x.T, ( h - self.y ) ) + (float(self.lambd)/m) * zrd \n",
    "\n",
    "\t#This is batch\n",
    "    def descent(self):\n",
    "        for i in range(self.iter):\n",
    "            self.cost.append( self.cost_fn(self.x.shape[0]))\n",
    "            gradientz = self.gradients(x.shape[0])\n",
    "\n",
    "\t\t\t#Change theta based on the \"gradientz\"\n",
    "            self.theta[0, :] = gradientz[0, :] - self.alpha * gradientz[0, :]\n",
    "            self.theta[1, :] = gradientz[1:, :] - self.alpha * gradientz[1:, :]\n",
    "\n",
    "        pred = np.dot(self.x, self.theta)\n",
    "        pred[ pred >= 0.5 ] = 1\n",
    "        pred[ pred < 0.5 ] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_func(theta, x):\n",
    "    return float(1) / (1 + math.e**(-x.dot(theta)))\n",
    "def log_gradient(theta, x, y):\n",
    "    first_calc = logistic_func(theta, x) - np.squeeze(y)\n",
    "    final_calc = first_calc.T.dot(x)\n",
    "    return final_calc\n",
    "def cost_func(theta, x, y):\n",
    "    log_func_v = logistic_func(theta,x)\n",
    "    y = np.squeeze(y)\n",
    "    step1 = y * np.log(log_func_v)\n",
    "    step2 = (1-y) * np.log(1 - log_func_v)\n",
    "    final = -step1 - step2\n",
    "    return np.mean(final)\n",
    "def grad_desc(theta_values, X, y, lr=.001, converge_change=.001):\n",
    "    #normalize\n",
    "    X = (X - np.mean(X, axis=0)) / np.std(X, axis=0)\n",
    "    #setup cost iter\n",
    "    cost_iter = []\n",
    "    cost = cost_func(theta_values, X, y)\n",
    "    cost_iter.append([0, cost])\n",
    "    change_cost = 1\n",
    "    i = 1\n",
    "    while(change_cost > converge_change):\n",
    "        old_cost = cost\n",
    "        theta_values = theta_values - (lr * log_gradient(theta_values, X, y))\n",
    "        cost = cost_func(theta_values, X, y)\n",
    "        cost_iter.append([i, cost])\n",
    "        change_cost = old_cost - cost\n",
    "        i+=1\n",
    "    print(i)\n",
    "    return theta_values, np.array(cost_iter)\n",
    "def pred_values(theta, X, hard=True):\n",
    "    #normalize\n",
    "    X = (X - np.mean(X, axis=0)) / np.std(X, axis=0)\n",
    "    pred_prob = logistic_func(theta, X)\n",
    "    pred_value = np.where(pred_prob >= .5, 1, 0)\n",
    "    if hard:\n",
    "        return pred_value\n",
    "    return pred_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NotFittedError(Exception):\n",
    "    pass\n",
    "\n",
    "class LogisticClassifier1():\n",
    "    def __init__(self, l_rate = 0.3, threshold = 0.5, fit_called=False):\n",
    "        self.fit_called = fit_called\n",
    "        self.coef = 0\n",
    "        self.bias = 0\n",
    "        self.threshold = threshold\n",
    "        self.l_rate = l_rate\n",
    "    \n",
    "    def fit(self, xtrain, ytrain, n_epoch = 5):\n",
    "        self.fit_called = True\n",
    "        self.coef = np.array([0.0 for i in range(xtrain.shape[1])])\n",
    "        for epoch in range(n_epoch):\n",
    "                #sum_error = 0\n",
    "            xtrain = xtrain.sample(frac=1).reset_index(drop=True)\n",
    "            for index, row in enumerate(xtrain.values):\n",
    "                    #print(self.bias)\n",
    "                yhat = self.predict_one(row)\n",
    "                delta = (yhat - ytrain.iloc[index])\n",
    "                #print(delta)\n",
    "                self.bias -= self.l_rate*delta\n",
    "                self.coef -= self.l_rate*delta*row\n",
    "                   \n",
    "        return self.coef, self.bias\n",
    "    \n",
    "    def predict_one(self, row):\n",
    "        if not self.fit_called:\n",
    "            raise NotFittedError()\n",
    "        return 1.0 / (1.0 + np.exp(-self.coef.dot(row) - self.bias))\n",
    "    \n",
    "#     def predict(self, xtest_array):\n",
    "#         if not self.fit_called:\n",
    "#             raise NotFittedError()\n",
    "#         else:\n",
    "#             predictions = []\n",
    "#             for i in range(len(xtest_array)):\n",
    "#                 xtest = xtest_array.iloc[i]\n",
    "#                 prediction = (self.predict_one(xtest))\n",
    "#                 predictions.append(prediction)\n",
    "#             return np.array(predictions)\n",
    "        \n",
    "    def predict(self, X):\n",
    "        if not self.fit_called:\n",
    "            raise NotFittedError()\n",
    "        \n",
    "        X = (X - np.mean(X, axis=0)) / np.std(X, axis=0) \n",
    "        predictions = np.array(1.0 / (1.0 + np.exp(-self.coef.dot(X.T) - self.bias)))\n",
    "#         for i in range(len(xtest_array)):\n",
    "#             xtest = xtest_array.iloc[i]\n",
    "#             prediction = self.predict_one(xtest)\n",
    "#             #predictions.append(prediction)\n",
    "#             predictions = np.append(predictions, prediction)\n",
    "        #predictions = np.array(predictions)\n",
    "        #predict_mask = [0 if i<self.threshold else 1 for i in predictions]\n",
    "        #print(predictions)\n",
    "        predict_mask = predictions>self.threshold\n",
    "        return predict_mask*1\n",
    "            #return predictions\n",
    "\n",
    "    def score(self, x_test, y_test):\n",
    "        yact_ypred = list(zip(y_test, self.predict(x_test)))\n",
    "        TP = yact_ypred.count((1,1))/len(x_test)\n",
    "        TN = yact_ypred.count((0,0))/len(x_test)\n",
    "        FP = yact_ypred.count((0,1))/len(x_test)\n",
    "        FN = yact_ypred.count((1,0))/len(x_test)\n",
    "        return float(TP + TN)/float(TP + TN + FP + FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NotFittedError(Exception):\n",
    "    pass\n",
    "\n",
    "class LogisticClassifier1():\n",
    "    def __init__(self, l_rate = 0.3, threshold = 0.5, fit_called=False):\n",
    "        self.fit_called = fit_called\n",
    "        self.coef = 0\n",
    "        self.bias = 0\n",
    "        self.threshold = threshold\n",
    "        self.l_rate = l_rate\n",
    "    \n",
    "    def fit(self, xtrain, ytrain, n_epoch = 5):\n",
    "        self.fit_called = True\n",
    "        \n",
    "        #normalize the data\n",
    "        xtrain = (xtrain - np.mean(xtrain, axis=0)) / np.std(xtrain, axis=0)\n",
    "        self.coef = np.array([0.0 for i in range(xtrain.shape[1])])\n",
    "        error = self.cost_func(xtrain, ytrain)\n",
    "        print(error)\n",
    "        i = 0\n",
    "        for epoch in range(n_epoch):\n",
    "            i += 1\n",
    "            old_sum = error\n",
    "            xtrain = xtrain.sample(frac=1).reset_index(drop=True)\n",
    "            for index, row in enumerate(xtrain.values):\n",
    "                    #print(self.bias)\n",
    "                yhat = self.predict_probability(row)\n",
    "                delta = (yhat - ytrain.iloc[index])\n",
    "                #print(delta)\n",
    "                self.bias -= self.l_rate*delta\n",
    "                self.coef -= self.l_rate*delta*row\n",
    "            error = self.cost_func(xtrain, ytrain)\n",
    "            #print(sum)\n",
    "            print(old_sum - error)\n",
    "            if abs(old_sum - error) <0.005:\n",
    "                return (i, self.bias, self.coef)  \n",
    "        return i, self.coef, self.bias\n",
    "    \n",
    "    def predict_probability(self, row):\n",
    "        if not self.fit_called:\n",
    "            raise NotFittedError()\n",
    "        return 1.0 / (1.0 + np.exp(-row.dot(self.coef) - self.bias))\n",
    "    \n",
    "\n",
    "    def cost_func(self, xtrain, ytrain):\n",
    "        log_func_v = self.predict_probability(xtrain)\n",
    "        y = ytrain\n",
    "        step1 = y * np.log(log_func_v)\n",
    "        step2 = (1-y) * np.log(1 - log_func_v)\n",
    "        final = -step1 - step2\n",
    "        return np.mean(final)\n",
    "        \n",
    "    def predict(self, X):\n",
    "        if not self.fit_called:\n",
    "            raise NotFittedError()\n",
    "        \n",
    "        X = (X - np.mean(X, axis=0)) / np.std(X, axis=0) \n",
    "        predictions = self.predict_probability(X)\n",
    "        #predictions = np.array(1.0 / (1.0 + np.exp(-X.dot(self.coef) - self.bias)))\n",
    "        #print(predictions)\n",
    "        pred_value = np.where(predictions >= .5, 1, 0)\n",
    "        return pred_value\n",
    "        \n",
    "\n",
    "    def score(self, x_test, y_test):\n",
    "        yact_ypred = list(zip(y_test, self.predict(x_test)))\n",
    "        TP = yact_ypred.count((1,1))/len(x_test)\n",
    "        TN = yact_ypred.count((0,0))/len(x_test)\n",
    "        FP = yact_ypred.count((0,1))/len(x_test)\n",
    "        FN = yact_ypred.count((1,0))/len(x_test)\n",
    "        return float(TP + TN)/float(TP + TN + FP + FN)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
